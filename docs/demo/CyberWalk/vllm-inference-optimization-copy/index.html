<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>CYBERWALK</title>
    <link rel="stylesheet" href="https://salif.github.io/zola-themes-collection/demo/CyberWalk/style.css">
</head>
<body>

    <nav class="main-menu">
        <button class="walkthrough-btn" onclick="startWalkthrough(1)">Slow Walk</button>
        <button class="walkthrough-btn" onclick="startWalkthrough(2)">Fast Scan</button>
        <button class="walkthrough-btn" onclick="location.href='/'">Root /</button>
    </nav>

    <div class="container">
        
<article class="post-detail">
    <header class="post-header">
        <h1 class="post-title">Optimizing Inference: The Rise of vLLM</h1>
        
        <div class="system-info glow-box">
            <p>> **STATUS**: [ ARCHIVED ]</p>
            <p>> **DATE_CREATED**: [ 2023-11-02 00:00:00 ]</p>
            <p>> **AUTHOR**: [ SYSTEM ]</p>
            <p>> **CATEGORY**: [ 
                
                    <a href="https://salif.github.io/zola-themes-collection/demo/CyberWalk/categories/sys-admin/">/SYS&#x2F;ADMIN</a> | 
                
                    <a href="https://salif.github.io/zola-themes-collection/demo/CyberWalk/categories/bin-ref/">/BIN&#x2F;REF</a>
                
            ]</p>
            <p>> **TAGS**: [
                
                    #inference, 
                
                    #cuda, 
                
                    #optimization, 
                
                    #gpu
                
            ]</p>
        </div>
    </header>

    <section class="post-content">
        <p>In the world of Large Language Models, <strong>throughput</strong> is money. The bottleneck in serving LLMs is rarely compute; it is memory bandwidth.</p>
<h2 id="the-memory-fragmentation-problem">The Memory Fragmentation Problem</h2>
<p>Standard attention mechanisms require contiguous memory blocks for Key-Value (KV) caches. As sequences grow, memory fragmentation occurs, leading to wasted VRAM (Video RAM). If you have 20% wasted VRAM, that's 20% fewer requests you can batch concurrently.</p>
<h2 id="enter-pagedattention-vllm">Enter PagedAttention (vLLM)</h2>
<p>Inspired by OS virtual memory pagination, vLLM allows the KV cache to be split into non-contiguous blocks. This allows for near-zero memory waste.</p>
<h3 id="implementation-benchmark">Implementation Benchmark</h3>
<p>Deploying a Llama-3-8B model using standard HuggingFace pipelines vs vLLM shows drastic differences.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Standard HuggingFace Pipeline (Slow)
</span><span style="color:#b48ead;">from </span><span>transformers </span><span style="color:#b48ead;">import </span><span>pipeline
</span><span>pipe = </span><span style="color:#bf616a;">pipeline</span><span>(&quot;</span><span style="color:#a3be8c;">text-generation</span><span>&quot;, </span><span style="color:#bf616a;">model</span><span>=&quot;</span><span style="color:#a3be8c;">meta-llama/Llama-3-8b</span><span>&quot;)
</span><span style="color:#65737e;"># ~ 30 tokens/sec on A100
</span><span>
</span><span style="color:#65737e;"># vLLM Engine (Fast)
</span><span style="color:#b48ead;">from </span><span>vllm </span><span style="color:#b48ead;">import </span><span style="color:#bf616a;">LLM</span><span>, SamplingParams
</span><span>llm = </span><span style="color:#bf616a;">LLM</span><span>(</span><span style="color:#bf616a;">model</span><span>=&quot;</span><span style="color:#a3be8c;">meta-llama/Llama-3-8b</span><span>&quot;)
</span><span style="color:#65737e;"># ~ 85 tokens/sec on A100 (due to better batching)
</span><span>Continuous Batching
</span><span>Traditional batching waits </span><span style="color:#b48ead;">for </span><span style="color:#96b5b4;">all </span><span>requests </span><span style="color:#b48ead;">in </span><span>a batch to finish before sending the response. If one request asks </span><span style="background-color:#bf616a;color:#2b303b;">for</span><span> </span><span style="color:#d08770;">500 </span><span>tokens and another </span><span style="color:#b48ead;">for</span><span> 5 tokens, the short request </span><span style="background-color:#bf616a;color:#2b303b;">is</span><span> blocked by the long one.
</span><span>
</span><span>vLLM implements Continuous </span><span style="color:#bf616a;">Batching </span><span>(iteration-level scheduling). As soon </span><span style="background-color:#bf616a;color:#2b303b;">as</span><span> a sequence finishes, a new one is inserted into the batch immediately.
</span><span>
</span><span>The Infrastructure Stack
</span><span>For a production-grade Python </span><span style="color:#bf616a;">AI </span><span>backend in </span><span style="color:#d08770;">2025</span><span>, the standard stack is becoming:
</span><span>
</span><span>FastAPI </span><span style="color:#b48ead;">for </span><span>the web layer.
</span><span>
</span><span>vLLM or </span><span style="color:#bf616a;">TGI </span><span>(Text Generation Inference) </span><span style="color:#b48ead;">for </span><span>the engine.
</span><span>
</span><span>Prometheus </span><span style="color:#b48ead;">for </span><span>monitoring token/sec latency.
</span><span>
</span><span>Stop serving models </span><span style="color:#b48ead;">with </span><span>raw torch.</span></code></pre>

    </section>

    <footer class="post-footer">
        <hr class="glow-divider">
        <p class="return-prompt">
            <a href="/">
                > cd ../.. <br>
                > RETURN TO ROOT_SYSTEM
            </a>
        </p>
    </footer>

</article>

    </div>

    <div id="modal-overlay"></div>
    <div id="qv-modal">
        <span class="close-btn">&times;</span>
        <div id="qv-content"></div>
    </div>

    <script src="https://salif.github.io/zola-themes-collection/demo/CyberWalk/js/script.js"></script>
</body>
</html>