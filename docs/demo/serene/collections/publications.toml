[[collection]]
type = "card"
feature = true
link = "https://example.com"
title = "Attention Is All You Need"
subtitle = "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, ..."
content = """\
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

*arXiv:1706.03762v7 [cs.CL] 02 Aug 2023*
"""

[[collection]]
type = "card"
link = "https://example.com"
title = "Deep Residual Learning for Image Recognition"
subtitle = "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
content = """\
Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.

*arXiv:1512.03385v1 [cs.CV] 10 Dec 2015*
"""

[[collection]]
type = "card"
link = "https://example.com"
title = "Generative Adversarial Networks"
subtitle = "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio"
content = """\
We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake.

*arXiv:1406.2661v1 [stat.ML] 10 Jun 2014*
"""
